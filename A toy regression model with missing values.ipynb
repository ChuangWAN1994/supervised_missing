{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We consider a simple regression problem where $X$ is bivariate gaussian, and $y$ is a linear function of the first coordinate, with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_without_missing_values(n_samples):\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, 0.5], [0.5, 1]]\n",
    "    X = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    \n",
    "    epsilon = 0.1 * np.random.randn(n_samples)\n",
    "    y = X[:, 0] + epsilon\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_without_missing_values(500)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y);\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "clb = plt.colorbar()\n",
    "clb.ax.set_title(\"y\");\n",
    "plt.title(\"Distribution of the data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "In order to evaluate an estimator, we use the learning curve: the evolution of the (cross-validated) prediction score, in function of the training size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve_without_missing_values(estimator):\n",
    "    X, y = generate_without_missing_values(10 ** 7)\n",
    "    sizes = np.logspace(2, 4, 5).astype(int)\n",
    "    \n",
    "    train_sizes, _, valid_scores = learning_curve(\n",
    "        estimator, X, y, train_sizes=sizes, scoring='r2', cv=30, random_state=0)\n",
    "\n",
    "    plt.plot(train_sizes, np.median(valid_scores, axis=1))\n",
    "    plt.fill_between(train_sizes,\n",
    "        np.percentile(valid_scores, q=25, axis=1),\n",
    "        np.percentile(valid_scores, q=75, axis=1), alpha=0.2)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Sample size')\n",
    "    plt.ylabel('r2 score') \n",
    "    plt.xlim(train_sizes.min(), train_sizes.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(min_samples_leaf=10)\n",
    "boosting = HistGradientBoostingRegressor()\n",
    "\n",
    "%time plot_learning_curve_without_missing_values(tree)\n",
    "plt.title(\"Learning curve for the decision tree\");\n",
    "plt.show();\n",
    "plt.clf();\n",
    "%time plot_learning_curve_without_missing_values(boosting)\n",
    "plt.title(\"Learning curve for the boosting\");\n",
    "plt.show();\n",
    "plt.clf();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation: different mechanisms\n",
    "\n",
    "In the following, we consider two missing data mechanisms: values missing completely at random (on both variables), and values missing only for high values of each variable (censor data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n_samples, mechanism):\n",
    "    X, y = generate_without_missing_values(n_samples)\n",
    "    \n",
    "    if mechanism == 'mcar':\n",
    "        missing_rate = 0.2\n",
    "        M = np.random.binomial(1, missing_rate, (n_samples, 2))\n",
    "    elif mechanism == 'censored':\n",
    "        M = (X > 0.5)\n",
    "    \n",
    "    np.putmask(X, M, np.nan)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different models to try\n",
    "\n",
    "We compare four missing data methods: imputation by the mean and iterative imputation (each variable is regressed from the other), with or without adding the missing data indicator. This is done both for simple trees and gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tree_with_mean_imputation = Pipeline([\n",
    "    ('Mean imputation', SimpleImputer(strategy='mean')), \n",
    "    ('Tree', DecisionTreeRegressor(min_samples_leaf=10))\n",
    "])   \n",
    "tree_with_iterative_imputation = Pipeline([\n",
    "    ('Iterative imputation', IterativeImputer()), \n",
    "    ('Tree', DecisionTreeRegressor(min_samples_leaf=10))\n",
    "])\n",
    "tree_with_mean_imputation_and_mask = Pipeline([\n",
    "    ('Mean imputation and mask', SimpleImputer(strategy='mean', add_indicator=True)), \n",
    "    ('Tree', DecisionTreeRegressor(min_samples_leaf=10))\n",
    "])   \n",
    "tree_with_iterative_imputation_and_mask = Pipeline([\n",
    "    ('Iterative imputation and mask', IterativeImputer(add_indicator=True)), \n",
    "    ('Tree', DecisionTreeRegressor(min_samples_leaf=10))\n",
    "])\n",
    "\n",
    "trees = {\n",
    "    'Mean': tree_with_mean_imputation,\n",
    "    'Iterative': tree_with_iterative_imputation,\n",
    "    'Mean + mask': tree_with_mean_imputation_and_mask,\n",
    "    'Iterative + mask': tree_with_iterative_imputation_and_mask\n",
    "}\n",
    "\n",
    "boosting_with_mean_imputation = Pipeline([\n",
    "    ('Mean imputation', SimpleImputer(strategy='mean')), \n",
    "    ('Boosting', HistGradientBoostingRegressor())\n",
    "])   \n",
    "boosting_with_iterative_imputation = Pipeline([\n",
    "    ('Iterative imputation', IterativeImputer()), \n",
    "    ('Boosting', HistGradientBoostingRegressor())\n",
    "])\n",
    "boosting_with_mean_imputation_and_mask = Pipeline([\n",
    "    ('Mean imputation and mask', SimpleImputer(strategy='mean', add_indicator=True)), \n",
    "    ('Boosting', HistGradientBoostingRegressor())\n",
    "])   \n",
    "boosting_with_iterative_imputation_and_mask = Pipeline([\n",
    "    ('Iterative imputation and mask', IterativeImputer(add_indicator=True)), \n",
    "    ('Boosting', HistGradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "boostings = {\n",
    "    'Mean': boosting_with_mean_imputation,\n",
    "    'Iterative': boosting_with_iterative_imputation,\n",
    "    'Mean+mask': boosting_with_mean_imputation_and_mask,\n",
    "    'Iterative+mask': boosting_with_iterative_imputation_and_mask\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We still use the learning curve, as well as a boxplot which is easier to read, extracting one slice of the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimators, mechanism):\n",
    "    X, y = generate(10 ** 7, mechanism)\n",
    "    sizes = np.logspace(2, 4, 5).astype(int)\n",
    "    \n",
    "    for key, est in estimators.items():\n",
    "        train_sizes, _, valid_scores = learning_curve(\n",
    "            est, X, y, train_sizes=sizes, scoring='r2', cv=30, random_state=0)\n",
    "        \n",
    "        plt.plot(train_sizes, np.median(valid_scores, axis=1), label=key)\n",
    "        plt.fill_between(train_sizes,\n",
    "            np.percentile(valid_scores, q=25, axis=1),\n",
    "            np.percentile(valid_scores, q=75, axis=1), alpha=0.2)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sample size')\n",
    "    plt.ylabel('r2 score') \n",
    "    plt.xlim(train_sizes.min(), train_sizes.max())\n",
    "    \n",
    "\n",
    "def boxplot_scores(estimators, n_samples, mechanism):\n",
    "    # one slice of the learning curve\n",
    "    X, y = generate(10 ** 6, mechanism)\n",
    "    \n",
    "    scores = {}\n",
    "    for key, est in estimators.items():\n",
    "        cv = ShuffleSplit(n_splits=30, train_size=n_samples, random_state=0)\n",
    "        scores[key] = cross_val_score(\n",
    "            est, X, y, scoring='r2', cv=cv)\n",
    "    \n",
    "    plt.boxplot(scores.values(), labels=scores.keys(), vert=False)\n",
    "    plt.title('{}, sample size: {}'.format(mechanism, n_samples))\n",
    "    plt.xlabel('r2 score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "When data is missing completely at random, iterative imputation (with ot without the mask) has better finite sample properties than mean imputation: it converges faster. However, for censor data, iterative imputation performs poorly without the help of the indicator, while mean imputation seems self-sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_learning_curve(trees, 'mcar')\n",
    "plt.title(\"Learning curve of the decision tree for mcar data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_scores(trees, 500, 'mcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_learning_curve(trees, 'censored')\n",
    "plt.title(\"Learning curve of the decision tree for censored data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_scores(trees, 500, 'censored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_learning_curve(boostings, 'mcar')\n",
    "plt.title(\"Learning curve of the boosting for mcar data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_scores(boostings, 200, 'mcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_learning_curve(boostings, 'censored')\n",
    "plt.title(\"Learning curve of the boosting for censored data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_scores(boostings, 200, 'censored')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
